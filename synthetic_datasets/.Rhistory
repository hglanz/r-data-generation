n_redundant = 2, n_repeated = 0, n_classes = 2,
n_clusters_per_class = 2, class_sep = 1.0, flip_y = 0.01,
weights = NULL, random_state = NULL, shuffle = TRUE, scale = 1.0, plot = FALSE) {
if (!is.null(random_state)) {
set.seed(random_state)
}
# check total features
total_features = n_informative + n_redundant + n_repeated
if (total_features > n_features) {
stop("Total number of informative, redundant, and repeated features must be <= n_features")
}
# random data values
informative_features <- matrix(rnorm(n_samples * n_informative), nrow=n_samples, ncol=n_informative)
# generate redundant features: linear combinations of informative features
if (n_redundant > 0) {
# n_informative * n_redundant random values bet 0-1 using runif
coeffs <- matrix(runif(n_informative * n_redundant), ncol = n_redundant)
redundant_features <- informative_features %*% coeffs
} else {
redundant_features <- NULL
}
# combine features and possibly add repeated features
X <- cbind(informative_features, redundant_features)
if (n_repeated > 0) {
#  indices for repeated features randomly from both informative and redundant features.
repeated_indices <- sample(n_informative + n_redundant, n_repeated, replace = TRUE)
repeated_features <- X[, repeated_indices]
X <- cbind(X, repeated_features)
}
# scale features
X <- X * scale
# generate labels with class separation
# according to the specified number of classes and their distribution (weights).
labels <- sample(0:(n_classes-1), n_samples, replace = TRUE, prob = weights)
# sampling indices by random with the number fraction flip_y* n_samples
flip_indices <- sample(n_samples, size = round(flip_y * n_samples))
# replace with another randomly sampled class
labels[flip_indices] <- sample(0:(n_classes-1), length(flip_indices), replace = TRUE)
# shuffle if requested
if (shuffle) {
indices <- sample(n_samples)
X <- X[indices, ]
labels <- labels[indices]
}
# plot if needed
if (plot && n_features >= 2) {
# 2d plotting
} else if (plot) {
warning("Plotting requires at least 2 features.")
}
return(list("X" = X, "labels" = labels))
}
# plotting and testing
# Test cases
# Test Case 1: default settings
make_classification(plot=TRUE)
make_classification <- function(n_samples = 100, n_features = 20, n_informative = 2,
n_redundant = 2, n_repeated = 0, n_classes = 2,
n_clusters_per_class = 2, class_sep = 1.0, flip_y = 0.01,
weights = NULL, random_state = NULL, shuffle = TRUE, scale = 1.0, plot = FALSE) {
if (!is.null(random_state)) {
set.seed(random_state)
}
# check total features
total_features = n_informative + n_redundant + n_repeated
if (total_features > n_features) {
stop("Total number of informative, redundant, and repeated features must be <= n_features")
}
# random data values
informative_features <- matrix(rnorm(n_samples * n_informative), nrow=n_samples, ncol=n_informative)
# generate redundant features: linear combinations of informative features
if (n_redundant > 0) {
# n_informative * n_redundant random values bet 0-1 using runif
coeffs <- matrix(runif(n_informative * n_redundant), ncol = n_redundant)
redundant_features <- informative_features %*% coeffs
} else {
redundant_features <- NULL
}
# combine features and possibly add repeated features
X <- cbind(informative_features, redundant_features)
if (n_repeated > 0) {
#  indices for repeated features randomly from both informative and redundant features.
repeated_indices <- sample(n_informative + n_redundant, n_repeated, replace = TRUE)
repeated_features <- X[, repeated_indices]
X <- cbind(X, repeated_features)
}
# scale features
X <- X * scale
# generate labels with class separation
# according to the specified number of classes and their distribution (weights).
labels <- sample(0:(n_classes-1), n_samples, replace = TRUE, prob = weights)
# sampling indices by random with the number fraction flip_y* n_samples
flip_indices <- sample(n_samples, size = round(flip_y * n_samples))
# replace with another randomly sampled class
labels[flip_indices] <- sample(0:(n_classes-1), length(flip_indices), replace = TRUE)
# shuffle if requested
if (shuffle) {
indices <- sample(n_samples)
X <- X[indices, ]
labels <- labels[indices]
}
# plot if needed
if (plot && n_features >= 2) {
# 2d plotting
} else if (plot) {
warning("Plotting requires at least 2 features.")
}
return(list("X" = X, "labels" = labels))
}
# plotting and testing
# Test cases
# Test Case 1: default settings
make_classification(plot=TRUE)
make_classification <- function(n_samples = 100, n_features = 20, n_informative = 2,
n_redundant = 2, n_repeated = 0, n_classes = 2,
n_clusters_per_class = 2, class_sep = 1.0, flip_y = 0.01,
weights = NULL, random_state = NULL, shuffle = TRUE, scale = 1.0, plot = FALSE) {
if (!is.null(random_state)) {
set.seed(random_state)
}
# check total features
total_features = n_informative + n_redundant + n_repeated
if (total_features > n_features) {
stop("Total number of informative, redundant, and repeated features must be <= n_features")
}
# random data values
informative_features <- matrix(rnorm(n_samples * n_informative), nrow=n_samples, ncol=n_informative)
# generate redundant features: linear combinations of informative features
if (n_redundant > 0) {
# n_informative * n_redundant random values bet 0-1 using runif
coeffs <- matrix(runif(n_informative * n_redundant), ncol = n_redundant)
redundant_features <- informative_features %*% coeffs
} else {
redundant_features <- NULL
}
# combine features and possibly add repeated features
X <- cbind(informative_features, redundant_features)
if (n_repeated > 0) {
#  indices for repeated features randomly from both informative and redundant features.
repeated_indices <- sample(n_informative + n_redundant, n_repeated, replace = TRUE)
repeated_features <- X[, repeated_indices]
X <- cbind(X, repeated_features)
}
# scale features
X <- X * scale
# generate labels with class separation
# according to the specified number of classes and their distribution (weights).
labels <- sample(0:(n_classes-1), n_samples, replace = TRUE, prob = weights)
# sampling indices by random with the number fraction flip_y* n_samples
flip_indices <- sample(n_samples, size = round(flip_y * n_samples))
# replace with another randomly sampled class
labels[flip_indices] <- sample(0:(n_classes-1), length(flip_indices), replace = TRUE)
# shuffle if requested
if (shuffle) {
indices <- sample(n_samples)
X <- X[indices, ]
labels <- labels[indices]
}
# plot if needed
if (plot && n_features >= 2) {
# 2d plotting
} else if (plot) {
warning("Plotting requires at least 2 features.")
}
return(list("X" = X, "labels" = labels))
}
# plotting and testing
# Test cases
# Test Case 1: default settings
make_classification(plot=TRUE)
make_circles <- function(n_samples = 100, shuffle = TRUE, noise = NULL, random_state = NULL, factor = 0.8, plot = FALSE) {
if (!is.null(random_state)) {
set.seed(random_state)
}
# sampling in radians
t <- seq(0, 2*pi, length.out = n_samples / 2 + 1)
t <- t[-length(t)]
# https://www.reddit.com/r/MachineLearning/comments/20gfyy/clustering_points_around_a_circle/
circle_outer <- cbind(cos(t), sin(t))
# a scaled version of the outer circle
circle_inner <- factor * circle_outer
# Combine the circles
data <- rbind(circle_outer, circle_inner)
# adding noise
if (!is.null(noise)) {
data <- data + matrix(rnorm(2 * n_samples, sd = noise), ncol = 2)
}
# shuffle
if (shuffle) {
data <- data[sample(n_samples), ]
}
# create labels 1 and 2 for the two classes (already in order so we can just assign down the middle)
labels <- c(rep(1, n_samples / 2), rep(2, n_samples / 2))
if (plot) {
plot(data, col = labels, asp = 1, xlab = "Feature 1", ylab = "Feature 2", pch = 16)
legend("topright", legend = c("Outer Circle", "Inner Circle"), col = 1:2 ,pch = 16 )
}
return(list("data" = data, "labels" = labels))
}
result <- make_circles(plot = TRUE, shuffle = FALSE, factor = 0.5, noise = 0.2)
make_moons <- function(n_samples = 100, shuffle = TRUE, noise = NULL, random_state = NULL, plot = FALSE) {
if (!is.null(random_state)) {
set.seed(random_state)
}
# Generate semicircle for the first moon
t1 <- seq(0, pi, length.out = n_samples / 2)
moon1_x <- cos(t1)
moon1_y <- sin(t1)
# Generate semicircle for the second moon, shifted and flipped
t2 <- seq(0, pi, length.out = n_samples / 2)
moon2_x <- 1 - cos(t2) # Shift to the right
moon2_y <- 1 - sin(t2) - 0.5 # Flip vertically and shift downwards
# Combine the moons
data <- rbind(cbind(moon1_x, moon1_y), cbind(moon2_x, moon2_y))
# Adding noise
if (!is.null(noise)) {
data <- data + matrix(rnorm(n_samples * 2, sd = noise), ncol = 2)
}
# Shuffle data if specified
if (shuffle) {
shuffle_index <- sample(n_samples)
data <- data[shuffle_index, ]
}
# Labels for the moons
labels <- c(rep(1, n_samples / 2), rep(2, n_samples / 2))
if (shuffle) {
labels <- labels[shuffle_index]
}
# Plot if specified
if (plot) {
plot(data, col = labels, xlab = "Feature 1", ylab = "Feature 2, pch = 19)
legend("topright", legend = c("Moon 1", "Moon 2"), col = 1:2, pch = 19)
make_moons <- function(n_samples = 100, shuffle = TRUE, noise = NULL, random_state = NULL, plot = FALSE) {
if (!is.null(random_state)) {
set.seed(random_state)
}
# Generate semicircle for the first moon
t1 <- seq(0, pi, length.out = n_samples / 2)
moon1_x <- cos(t1)
moon1_y <- sin(t1)
# Generate semicircle for the second moon, shifted and flipped
t2 <- seq(0, pi, length.out = n_samples / 2)
moon2_x <- 1 - cos(t2) # Shift to the right
moon2_y <- 1 - sin(t2) - 0.5 # Flip vertically and shift downwards
# Combine the moons
data <- rbind(cbind(moon1_x, moon1_y), cbind(moon2_x, moon2_y))
# Adding noise
if (!is.null(noise)) {
data <- data + matrix(rnorm(n_samples * 2, sd = noise), ncol = 2)
}
# Shuffle data if specified
if (shuffle) {
shuffle_index <- sample(n_samples)
data <- data[shuffle_index, ]
}
# Labels for the moons
labels <- c(rep(1, n_samples / 2), rep(2, n_samples / 2))
if (shuffle) {
labels <- labels[shuffle_index]
}
# Plot if specified
if (plot) {
plot(data, col = labels, xlab = "Feature 1", ylab = "Feature 2",pch = 19)
legend("topright", legend = c("Moon 1", "Moon 2"), col = 1:2, pch = 19)
}
return(list("X" = data, "y" = labels))
}
make_moons(n_samples = 500, plot = TRUE, shuffle = FALSE)
# Moon generation with noise
make_moons(n_samples = 200, noise = 0.1, plot = TRUE, shuffle = FALSE)
# Moon generation with a specific random state for reproducibility
make_moons(n_samples = 200, noise = 0.23, random_state = 42, plot = TRUE)
make_circles <- function(n_samples = 100, shuffle = TRUE, noise = NULL, random_state = NULL, factor = 0.8, plot = FALSE) {
if (!is.null(random_state)) {
set.seed(random_state)
}
# sampling in radians
t <- seq(0, 2*pi, length.out = n_samples / 2 + 1)
t <- t[-length(t)]
# https://www.reddit.com/r/MachineLearning/comments/20gfyy/clustering_points_around_a_circle/
circle_outer <- cbind(cos(t), sin(t))
# a scaled version of the outer circle
circle_inner <- factor * circle_outer
# Combine the circles
data <- rbind(circle_outer, circle_inner)
# adding noise
if (!is.null(noise)) {
data <- data + matrix(rnorm(2 * n_samples, sd = noise), ncol = 2)
}
# shuffle
if (shuffle) {
data <- data[sample(n_samples), ]
}
# create labels 1 and 2 for the two classes (already in order so we can just assign down the middle)
labels <- c(rep(1, n_samples / 2), rep(2, n_samples / 2))
if (plot) {
plot(data, col = labels, asp = 1, xlab = "Feature 1", ylab = "Feature 2", pch = 16)
legend("topright", legend = c("Outer Circle", "Inner Circle"), col = 1:2 ,pch = 16 )
}
return(list("data" = data, "labels" = labels))
}
result <- make_circles(plot = TRUE, shuffle = FALSE, factor = 0.5, noise = 0.2)
make_regression <- function(n_samples=100, n_features=100, n_informative=10, n_targets=1,
bias=0.0, noise=0.0, shuffle=TRUE, coef=FALSE, random_state=NULL,
effective_rank=NULL, tail_strength=0.5, plot = FALSE) {
if (!is.null(random_state)) {
set.seed(random_state)
}
# set the number of informative coefficients
coefficients <- rnorm(n_informative)
# random data values
# initial feature matrix (X) with more features than the effective rank
X <- matrix(rnorm(n_samples * n_features), nrow=n_samples, ncol=n_features)
# implementing effective rank and tail_strength
if (!is.null(effective_rank) && effective_rank < n_features) {
# decompose a matrix X into its singular vectors and singular values
svd_result <- svd(X)
U <- svd_result$u
Sigma <- svd_result$d
V <- svd_result$v
# for matrix X to have an effective_rank of k, keep the top k singular values and set the rest to zero.
# Reduces the rank of the matrix, simulating a situation where only a subset of features provides most information.
k <- effective_rank
Sigma_adjusted <- c(Sigma[1:k], rep(0, length(Sigma) - k))
#  linearly reducing the magnitude of each successive singular value from the largest to the kth value
# adjust each of the top k singular values by a factor that decreases linearly
decay_factor <- seq(from = 1, to = tail_strength, length.out = k)
Sigma_adjusted[1:k] <- Sigma_adjusted[1:k] * decay_factor
# reconstructing the matrix
X_adjusted <- U %*% diag(Sigma_adjusted) %*% t(V)
X <- X_adjusted
}
# target data. Y = intercept + coeff * X + error
informative_X <- X[, seq(n_informative)]
if (n_targets == 1) {
Y <- matrix(nrow=n_samples, ncol=1)
} else {
Y <- matrix(nrow=n_samples, ncol=n_targets)
}
for (target in 1:n_targets) {
Y[, target] <- bias + informative_X %*% coefficients + rnorm(n_samples, sd=noise)
}
# multi-output check
if (n_targets > 1) {
Y <- matrix(Y, ncol=n_targets)
}
# shuffle if requested
if (shuffle) {
indices <- sample(n_samples)
X <- X[indices, ]
Y <- Y[indices, ]
}
result <- if (coef) {
list("X"=X, "Y"=Y, "coefficients"=coefficients)
} else {
list("X"=X, "Y"=Y)
}
if (plot) {
if (n_targets == 1) {
plot(X[,1], Y, main="Feature 1 vs Target", xlab="Feature 1", ylab="Target")
} else {
par(mfrow=c(1, n_targets))
for (i in 1:n_targets) {
plot(X[,1], Y[,i], main=paste("Feature 1 vs Target", i), xlab="Feature 1", ylab=paste("Target", i))
}
}
}
return(result)
}
# Testing
# Test Case 1: Default Parameters
make_regression(plot=TRUE)
make_regression <- function(n_samples=100, n_features=100, n_informative=10, n_targets=1,
bias=0.0, noise=0.0, shuffle=TRUE, coef=FALSE, random_state=NULL,
effective_rank=NULL, tail_strength=0.5, plot = FALSE) {
if (!is.null(random_state)) {
set.seed(random_state)
}
# set the number of informative coefficients
coefficients <- rnorm(n_informative)
# random data values
# initial feature matrix (X) with more features than the effective rank
X <- matrix(rnorm(n_samples * n_features), nrow=n_samples, ncol=n_features)
# implementing effective rank and tail_strength
if (!is.null(effective_rank) && effective_rank < n_features) {
# decompose a matrix X into its singular vectors and singular values
svd_result <- svd(X)
U <- svd_result$u
Sigma <- svd_result$d
V <- svd_result$v
# for matrix X to have an effective_rank of k, keep the top k singular values and set the rest to zero.
# Reduces the rank of the matrix, simulating a situation where only a subset of features provides most information.
k <- effective_rank
Sigma_adjusted <- c(Sigma[1:k], rep(0, length(Sigma) - k))
#  linearly reducing the magnitude of each successive singular value from the largest to the kth value
# adjust each of the top k singular values by a factor that decreases linearly
decay_factor <- seq(from = 1, to = tail_strength, length.out = k)
Sigma_adjusted[1:k] <- Sigma_adjusted[1:k] * decay_factor
# reconstructing the matrix
X_adjusted <- U %*% diag(Sigma_adjusted) %*% t(V)
X <- X_adjusted
}
# target data. Y = intercept + coeff * X + error
informative_X <- X[, seq(n_informative)]
if (n_targets == 1) {
Y <- matrix(nrow=n_samples, ncol=1)
} else {
Y <- matrix(nrow=n_samples, ncol=n_targets)
}
for (target in 1:n_targets) {
Y[, target] <- bias + informative_X %*% coefficients + rnorm(n_samples, sd=noise)
}
# multi-output check
if (n_targets > 1) {
Y <- matrix(Y, ncol=n_targets)
}
# shuffle if requested
if (shuffle) {
indices <- sample(n_samples)
X <- X[indices, ]
Y <- Y[indices, ]
}
result <- if (coef) {
list("X"=X, "Y"=Y, "coefficients"=coefficients)
} else {
list("X"=X, "Y"=Y)
}
if (plot) {
if (n_targets == 1) {
plot(X[,1], Y, main="Feature 1 vs Target", xlab="Feature 1", ylab="Target")
} else {
par(mfrow=c(1, n_targets))
for (i in 1:n_targets) {
plot(X[,1], Y[,i], xlab="Feature 1", ylab=paste("Target", i))
}
}
}
return(result)
}
# Testing
# Test Case 1: Default Parameters
make_regression(plot=TRUE)
make_regression <- function(n_samples=100, n_features=100, n_informative=10, n_targets=1,
bias=0.0, noise=0.0, shuffle=TRUE, coef=FALSE, random_state=NULL,
effective_rank=NULL, tail_strength=0.5, plot = FALSE) {
if (!is.null(random_state)) {
set.seed(random_state)
}
# set the number of informative coefficients
coefficients <- rnorm(n_informative)
# random data values
# initial feature matrix (X) with more features than the effective rank
X <- matrix(rnorm(n_samples * n_features), nrow=n_samples, ncol=n_features)
# implementing effective rank and tail_strength
if (!is.null(effective_rank) && effective_rank < n_features) {
# decompose a matrix X into its singular vectors and singular values
svd_result <- svd(X)
U <- svd_result$u
Sigma <- svd_result$d
V <- svd_result$v
# for matrix X to have an effective_rank of k, keep the top k singular values and set the rest to zero.
# Reduces the rank of the matrix, simulating a situation where only a subset of features provides most information.
k <- effective_rank
Sigma_adjusted <- c(Sigma[1:k], rep(0, length(Sigma) - k))
#  linearly reducing the magnitude of each successive singular value from the largest to the kth value
# adjust each of the top k singular values by a factor that decreases linearly
decay_factor <- seq(from = 1, to = tail_strength, length.out = k)
Sigma_adjusted[1:k] <- Sigma_adjusted[1:k] * decay_factor
# reconstructing the matrix
X_adjusted <- U %*% diag(Sigma_adjusted) %*% t(V)
X <- X_adjusted
}
# target data. Y = intercept + coeff * X + error
informative_X <- X[, seq(n_informative)]
if (n_targets == 1) {
Y <- matrix(nrow=n_samples, ncol=1)
} else {
Y <- matrix(nrow=n_samples, ncol=n_targets)
}
for (target in 1:n_targets) {
Y[, target] <- bias + informative_X %*% coefficients + rnorm(n_samples, sd=noise)
}
# multi-output check
if (n_targets > 1) {
Y <- matrix(Y, ncol=n_targets)
}
# shuffle if requested
if (shuffle) {
indices <- sample(n_samples)
X <- X[indices, ]
Y <- Y[indices, ]
}
result <- if (coef) {
list("X"=X, "Y"=Y, "coefficients"=coefficients)
} else {
list("X"=X, "Y"=Y)
}
if (plot) {
if (n_targets == 1) {
plot(X[,1], Y, xlab="Feature 1", ylab="Target", pch = 19)
} else {
par(mfrow=c(1, n_targets))
for (i in 1:n_targets) {
plot(X[,1], Y[,i], xlab="Feature 1", ylab=paste("Target", i), pch = 19)
}
}
}
return(result)
}
# Testing
# Test Case 1: Default Parameters
make_regression(plot=TRUE)
