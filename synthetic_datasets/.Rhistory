plot(X[,1], Y, xlab="Feature 1", ylab="Target", pch = 19)
} else {
par(mfrow=c(1, n_targets))
for (i in 1:n_targets) {
plot(X[,1], Y[,i], xlab="Feature 1", ylab=paste("Target", i), pch = 19)
}
}
}
return(result)
}
# Testing
# Test Case 1: Default Parameters
make_regression(plot=TRUE)
make_circles <- function(n_samples = 100, shuffle = TRUE, noise = NULL, random_state = NULL, factor = 0.8, plot = FALSE) {
if (!is.null(random_state)) {
set.seed(random_state)
}
# sampling in radians
t <- seq(0, 2*pi, length.out = n_samples / 2 + 1)
t <- t[-length(t)]
# https://www.reddit.com/r/MachineLearning/comments/20gfyy/clustering_points_around_a_circle/
circle_outer <- cbind(cos(t), sin(t))
# a scaled version of the outer circle
circle_inner <- factor * circle_outer
# Combine the circles
data <- rbind(circle_outer, circle_inner)
# adding noise
if (!is.null(noise)) {
data <- data + matrix(rnorm(2 * n_samples, sd = noise), ncol = 2)
}
# shuffle
if (shuffle) {
data <- data[sample(n_samples), ]
}
# create labels 1 and 2 for the two classes (already in order so we can just assign down the middle)
labels <- c(rep(1, n_samples / 2), rep(2, n_samples / 2))
if (plot) {
plot(data, col = labels, asp = 1, xlab = "Feature 1", ylab = "Feature 2", pch = 16)
legend("topright", legend = c("Outer Circle", "Inner Circle"), col = 1:2 ,pch = 16 )
}
return(list("data" = data, "labels" = labels))
}
# Default settings
make_circles(plot = TRUE)
# Increased number of samples
make_circles(n_samples = 200, plot = TRUE)
# Adding noise
make_circles(noise = 0.1, plot = TRUE)
# Without shuffling
make_circles(shuffle = FALSE, plot = TRUE)
# Different factor for inner and outer circle
make_circles(factor = 0.5, plot = TRUE)
# High noise level
make_circles(noise = 0.3, plot = TRUE)
# Large number of samples with noise
make_circles(n_samples = 300, noise = 0.1, plot = TRUE)
# Small factor and high noise
make_circles(factor = 0.3, noise = 0.2, plot = TRUE)
# Large factor, no noise
make_circles(factor = 0.9, noise = NULL, plot = TRUE)
# Very small number of samples
make_circles(n_samples = 50, plot = TRUE)
make_moons <- function(n_samples = 100, shuffle = TRUE, noise = NULL, random_state = NULL, plot = FALSE) {
if (!is.null(random_state)) {
set.seed(random_state)
}
# generate semicircle for the first moon
t1 <- seq(0, pi, length.out = n_samples / 2)
moon1_x <- cos(t1)
moon1_y <- sin(t1)
# generate semicircle for the second moon
t2 <- seq(0, pi, length.out = n_samples / 2)
# shifted to the right
moon2_x <- 1 - cos(t2)
# flipped vertically and shift downwards
moon2_y <- 1 - sin(t2) - 0.5
# combine the moons
data <- rbind(cbind(moon1_x, moon1_y), cbind(moon2_x, moon2_y))
# adding noise
if (!is.null(noise)) {
data <- data + matrix(rnorm(n_samples * 2, sd = noise), ncol = 2)
}
# shuffle data if specified
if (shuffle) {
shuffle_index <- sample(n_samples)
data <- data[shuffle_index, ]
}
# labels for the moons
labels <- c(rep(1, n_samples / 2), rep(2, n_samples / 2))
if (shuffle) {
labels <- labels[shuffle_index]
}
# plot if specified
if (plot) {
plot(data, col = labels, xlab = "Feature 1", ylab = "Feature 2",pch = 19)
legend("topright", legend = c("Moon 1", "Moon 2"), col = 1:2, pch = 19)
}
return(list("X" = data, "y" = labels))
}
# Default settings
make_moons(plot = TRUE)
# Higher number of samples
make_moons(n_samples = 300, plot = TRUE)
# noise
make_moons(noise = 0.1, plot = TRUE)
# Increased noise
make_moons(noise = 0.3, plot = TRUE)
# No shuffle
make_moons(shuffle = FALSE, plot = TRUE)
# Specific random state for reproducibility
make_moons(random_state = 42, plot = TRUE)
# Large number of samples with noise
make_moons(n_samples = 500, noise = 0.1, plot = TRUE)
# Large number of samples, high noise, no shuffle
make_moons(n_samples = 500, noise = 0.2, shuffle = FALSE, plot = TRUE)
# Medium noise with a specific random state
make_moons(noise = 0.15, random_state = 24, plot = TRUE)
# High noise, no shuffle, specific random state
make_moons(noise = 0.25, shuffle = FALSE, random_state = 11, plot = TRUE)
# Default settings
make_classification(plot=TRUE)
# Increase informative features
make_classification(n_informative=4, plot=TRUE)
# High class separation
make_classification(class_sep=2.0, plot=TRUE)
# More samples, high noise (flip_y)
make_classification(n_samples=200, flip_y=0.1, plot=TRUE)
# More clusters per class
make_classification(n_clusters_per_class=3, plot=TRUE)
# Unbalanced class weights
make_classification(weights=c(0.7, 0.3), plot=TRUE)
# Only informative features
make_classification(n_redundant=0, n_repeated=0, plot=TRUE)
# Many features, mostly noise
make_classification(n_features=40, n_informative=2, plot=TRUE)
# Different scale
make_classification(scale=0.5, plot=TRUE)
make_classification <- function(n_samples = 100, n_features = 20, n_informative = 2,
n_redundant = 2, n_repeated = 0, n_classes = 2,
n_clusters_per_class = 2, class_sep = 1.0, flip_y = 0.01,
weights = NULL, random_state = NULL, shuffle = TRUE, scale = 1.0, plot = FALSE) {
if (!is.null(random_state)) {
set.seed(random_state)
}
# check total features
total_features = n_informative + n_redundant + n_repeated
if (total_features > n_features) {
stop("Total number of informative, redundant, and repeated features must be <= n_features")
}
# random data values
informative_features <- matrix(rnorm(n_samples * n_informative), nrow=n_samples, ncol=n_informative)
# generate redundant features: linear combinations of informative features
if (n_redundant > 0) {
# n_informative * n_redundant random values bet 0-1 using runif
coeffs <- matrix(runif(n_informative * n_redundant), ncol = n_redundant)
redundant_features <- informative_features %*% coeffs
} else {
redundant_features <- NULL
}
# combine features and possibly add repeated features
X <- cbind(informative_features, redundant_features)
if (n_repeated > 0) {
#  indices for repeated features randomly from both informative and redundant features.
repeated_indices <- sample(n_informative + n_redundant, n_repeated, replace = TRUE)
repeated_features <- X[, repeated_indices]
X <- cbind(X, repeated_features)
}
# scale features
X <- X * scale
# generate labels with class separation
# according to the specified number of classes and their distribution (weights).
labels <- sample(0:(n_classes-1), n_samples, replace = TRUE, prob = weights)
# sampling indices by random with the number fraction flip_y* n_samples
flip_indices <- sample(n_samples, size = round(flip_y * n_samples))
# replace with another randomly sampled class
labels[flip_indices] <- sample(0:(n_classes-1), length(flip_indices), replace = TRUE)
# shuffle if requested
if (shuffle) {
indices <- sample(n_samples)
X <- X[indices, ]
labels <- labels[indices]
}
# plot if needed
if (plot && n_features >= 2) {
# 2d plotting
} else if (plot) {
warning("Plotting requires at least 2 features.")
}
return(list("X" = X, "labels" = labels))
}
# plotting and testing
# Test cases
# Default settings
make_classification(plot=TRUE)
# Increase informative features
make_classification(n_informative=4, plot=TRUE)
# High class separation
make_classification(class_sep=2.0, plot=TRUE)
# More samples, high noise (flip_y)
make_classification(n_samples=200, flip_y=0.1, plot=TRUE)
# More clusters per class
make_classification(n_clusters_per_class=3, plot=TRUE)
# Unbalanced class weights
make_classification(weights=c(0.7, 0.3), plot=TRUE)
# Only informative features
make_classification(n_redundant=0, n_repeated=0, plot=TRUE)
# Many features, mostly noise
make_classification(n_features=40, n_informative=2, plot=TRUE)
# Different scale
make_classification(scale=0.5, plot=TRUE)
make_classification <- function(n_samples = 100, n_features = 20, n_informative = 2,
n_redundant = 2, n_repeated = 0, n_classes = 2,
n_clusters_per_class = 2, class_sep = 1.0, flip_y = 0.01,
weights = NULL, random_state = NULL, shuffle = TRUE, scale = 1.0, plot = FALSE) {
if (!is.null(random_state)) {
set.seed(random_state)
}
# check total features
total_features = n_informative + n_redundant + n_repeated
if (total_features > n_features) {
stop("Total number of informative, redundant, and repeated features must be <= n_features")
}
# random data values
informative_features <- matrix(rnorm(n_samples * n_informative), nrow=n_samples, ncol=n_informative)
# generate redundant features: linear combinations of informative features
if (n_redundant > 0) {
# n_informative * n_redundant random values bet 0-1 using runif
coeffs <- matrix(runif(n_informative * n_redundant), ncol = n_redundant)
redundant_features <- informative_features %*% coeffs
} else {
redundant_features <- NULL
}
# combine features and possibly add repeated features
X <- cbind(informative_features, redundant_features)
if (n_repeated > 0) {
#  indices for repeated features randomly from both informative and redundant features.
repeated_indices <- sample(n_informative + n_redundant, n_repeated, replace = TRUE)
repeated_features <- X[, repeated_indices]
X <- cbind(X, repeated_features)
}
# scale features
X <- X * scale
# generate labels with class separation
# according to the specified number of classes and their distribution (weights).
labels <- sample(0:(n_classes-1), n_samples, replace = TRUE, prob = weights)
# sampling indices by random with the number fraction flip_y* n_samples
flip_indices <- sample(n_samples, size = round(flip_y * n_samples))
# replace with another randomly sampled class
labels[flip_indices] <- sample(0:(n_classes-1), length(flip_indices), replace = TRUE)
# shuffle if requested
if (shuffle) {
indices <- sample(n_samples)
X <- X[indices, ]
labels <- labels[indices]
}
# plot if needed
if (plot && n_features >= 2) {
colors <- c('red', 'blue', 'green', 'yellow', 'purple', 'orange')[1:n_classes]
plot(X[,1], X[,2], col=colors[labels + 1], xlab="Feature 1", ylab="Feature 2", main="Classification Plot")
legend("topright", legend=paste("Class", 0:(n_classes-1)), fill=colors[1:n_classes])
} else if (plot) {
warning("Plotting requires at least 2 features.")
}
return(list("X" = X, "labels" = labels))
}
# plotting and testing
# Test cases
# Default settings
make_classification(plot=TRUE)
# Increase informative features
make_classification(n_informative=4, plot=TRUE)
# High class separation
make_classification(class_sep=2.0, plot=TRUE)
# More samples, high noise (flip_y)
make_classification(n_samples=200, flip_y=0.1, plot=TRUE)
# More clusters per class
make_classification(n_clusters_per_class=3, plot=TRUE)
# Unbalanced class weights
make_classification(weights=c(0.7, 0.3), plot=TRUE)
# Only informative features
make_classification(n_redundant=0, n_repeated=0, plot=TRUE)
# Many features, mostly noise
make_classification(n_features=40, n_informative=2, plot=TRUE)
# Different scale
make_classification(scale=0.5, plot=TRUE)
make_classification <- function(n_samples = 100, n_features = 20, n_informative = 2,
n_redundant = 2, n_repeated = 0, n_classes = 2,
n_clusters_per_class = 2, class_sep = 1.0, flip_y = 0.01,
weights = NULL, random_state = NULL, shuffle = TRUE, scale = 1.0, plot = FALSE) {
if (!is.null(random_state)) {
set.seed(random_state)
}
# check total features
total_features = n_informative + n_redundant + n_repeated
if (total_features > n_features) {
stop("Total number of informative, redundant, and repeated features must be <= n_features")
}
# random data values
informative_features <- matrix(rnorm(n_samples * n_informative), nrow=n_samples, ncol=n_informative)
# generate redundant features: linear combinations of informative features
if (n_redundant > 0) {
# n_informative * n_redundant random values bet 0-1 using runif
coeffs <- matrix(runif(n_informative * n_redundant), ncol = n_redundant)
redundant_features <- informative_features %*% coeffs
} else {
redundant_features <- NULL
}
# combine features and possibly add repeated features
X <- cbind(informative_features, redundant_features)
if (n_repeated > 0) {
#  indices for repeated features randomly from both informative and redundant features.
repeated_indices <- sample(n_informative + n_redundant, n_repeated, replace = TRUE)
repeated_features <- X[, repeated_indices]
X <- cbind(X, repeated_features)
}
# scale features
X <- X * scale
# generate labels with class separation
# according to the specified number of classes and their distribution (weights).
labels <- sample(0:(n_classes-1), n_samples, replace = TRUE, prob = weights)
# sampling indices by random with the number fraction flip_y* n_samples
flip_indices <- sample(n_samples, size = round(flip_y * n_samples))
# replace with another randomly sampled class
labels[flip_indices] <- sample(0:(n_classes-1), length(flip_indices), replace = TRUE)
# shuffle if requested
if (shuffle) {
indices <- sample(n_samples)
X <- X[indices, ]
labels <- labels[indices]
}
# plot if needed
if (plot && n_features >= 2) {
colors <- c('red', 'blue', 'green', 'yellow', 'purple', 'orange')[1:n_classes]
plot(X[,1], X[,2], col=colors[labels + 1], xlab="Feature 1", ylab="Feature 2", main="Classification Plot", pch = 19)
legend("topright", legend=paste("Class", 0:(n_classes-1)), fill=colors[1:n_classes])
} else if (plot) {
warning("Plotting requires at least 2 features.")
}
return(list("X" = X, "labels" = labels))
}
# plotting and testing
# Test cases
# Default settings
make_classification(plot=TRUE)
# Increase informative features
make_classification(n_informative=4, plot=TRUE)
# High class separation
make_classification(class_sep=2.0, plot=TRUE)
# More samples, high noise (flip_y)
make_classification(n_samples=200, flip_y=0.1, plot=TRUE)
# More clusters per class
make_classification(n_clusters_per_class=3, plot=TRUE)
# Unbalanced class weights
make_classification(weights=c(0.7, 0.3), plot=TRUE)
# Only informative features
make_classification(n_redundant=0, n_repeated=0, plot=TRUE)
# Many features, mostly noise
make_classification(n_features=40, n_informative=2, plot=TRUE)
# Different scale
make_classification(scale=0.5, plot=TRUE)
make_blobs(plot=TRUE)
library(scatterplot3d)
make_blobs <- function(n_samples=100, n_features=2, centers=NULL, cluster_std=1.0,
center_box=c(-10, 10), shuffle=TRUE, random_state=NULL, return_centers=FALSE, plot=FALSE) {
if (!is.null(random_state)) {
set.seed(random_state)
}
if (is.null(centers)) {
n_centers <- 3
} else if (is.numeric(centers)) {
n_centers <- centers
centers <- matrix(runif(n_centers * n_features, min=center_box[1], max=center_box[2]), nrow=n_centers)
} else {
n_centers <- nrow(centers)
}
samples_per_center <- ceiling(n_samples / n_centers)
cluster_labels <- rep(1:n_centers, each=samples_per_center)[1:n_samples]
X <- matrix(nrow=n_samples, ncol=n_features)
for (i in 1:n_centers) {
center <- if (is.null(centers)) {
runif(n_features, min=center_box[1], max=center_box[2])
} else {
centers[i, ]
}
indices <- which(cluster_labels == i)
X[indices, ] <- mapply(rnorm, n=length(indices), mean=center, sd=cluster_std)
}
if (shuffle) {
indices <- sample(n_samples)
X <- X[indices, ]
cluster_labels <- cluster_labels[indices]
}
if (plot) {
if (n_features == 2) {
colors <- rainbow(n_centers)
plot(X, col=colors[cluster_labels], pch=19, xlab="Feature 1", ylab="Feature 2", main="Generated Blob Clusters")
if (return_centers) {
points(centers, pch=4, col=1:n_centers, lwd=2)
}
} else if (n_features == 3) {
colors <- rainbow(n_centers)
scatterplot3d(X[,1], X[,2], X[,3], color=colors[cluster_labels], pch=19, xlab="Feature 1", ylab="Feature 2", zlab="Feature 3", main="Generated Blob Clusters")
if (return_centers) {
scatterplot3d(centers[,1], centers[,2], centers[,3], color=1:n_centers, pch=4, lwd=2, add=TRUE)
}
} else {
warning("Plotting is available only for 2D and 3D data.")
}
}
if (return_centers) {
return(list("X"=X, "y"=cluster_labels, "centers"=centers))
} else {
return(list("X"=X, "y"=cluster_labels))
}
}
# Test Cases
# Test case 1: Default settings
make_blobs(plot=TRUE)
make_blobs(n_samples=200, centers=4, plot=TRUE)
# Test case 3: Custom cluster centers and higher variability
custom_centers <- matrix(c(0, 0, 5, 5, -5, -5), byrow=TRUE, ncol=2)
make_blobs(n_samples=150, centers=custom_centers, cluster_std=2, plot=TRUE, random_state=24)
# Test case 3: Custom cluster centers and higher variability
custom_centers <- matrix(c(-5, -5, 5, 5, 0, 10), ncol=2, byrow=TRUE)
custom_centers_3d <- matrix(c(-5, -5, 0, 5, 5, 0, 0, 10, 5), ncol=3, byrow=TRUE)
make_blobs(n_samples=150, centers=custom_centers, cluster_std=2, plot=TRUE, random_state=24)
make_blobs(n_samples=100, n_features=3, centers=3, plot=TRUE)
make_four_square_dataset <- function(n_samples=100, noise=0.0, shuffle=TRUE, random_state=NULL, plot=FALSE) {
if (!is.null(random_state)) {
set.seed(random_state)
}
n_samples_per_class <- n_samples / 4
# first class (bottom left and top right)
x1_class1_bottom_left <- runif(n_samples_per_class, min=0, max=0.5)
x2_class1_bottom_left <- runif(n_samples_per_class, min=0, max=0.5)
x1_class1_top_right <- runif(n_samples_per_class, min=0.5, max=1)
x2_class1_top_right <- runif(n_samples_per_class, min=0.5, max=1)
# second class (top left and bottom right)
x1_class2_top_left <- runif(n_samples_per_class, min=0, max=0.5)
x2_class2_top_left <- runif(n_samples_per_class, min=0.5, max=1)
x1_class2_bottom_right <- runif(n_samples_per_class, min=0.5, max=1)
x2_class2_bottom_right <- runif(n_samples_per_class, min=0, max=0.5)
features <- rbind(
cbind(x1_class1_bottom_left, x2_class1_bottom_left),
cbind(x1_class1_top_right, x2_class1_top_right),
cbind(x1_class2_top_left, x2_class2_top_left),
cbind(x1_class2_bottom_right, x2_class2_bottom_right)
)
features <- features + matrix(rnorm(n_samples * 2, mean = 0, sd = noise), ncol = 2)
target <- c(rep(1, n_samples_per_class * 2), rep(2, n_samples_per_class * 2))
# shuffle the dataset if requested
if (shuffle) {
indices <- sample(n_samples)
features <- features[indices, ]
target <- target[indices]
}
if (plot) {
colors <- c("red", "blue")
plot(features, col=colors[target], pch=19, xlab="Feature 1", ylab="Feature 2")
legend("topright", legend=c("Class 1", "Class 2"), fill=colors)
}
list("features" = features, "target" = target)
}
# perfect 4 squares
make_four_square_dataset(n_samples=1000, shuffle=FALSE, random_state=123, plot=TRUE)
# noise added
make_four_square_dataset(n_samples=1000, noise=0.1, shuffle=TRUE, random_state=123, plot=TRUE)
# larger dataset and lower noise
make_four_square_dataset(n_samples=2000, noise=0.05, shuffle=TRUE, random_state=123, plot=TRUE)
# high noise
make_four_square_dataset(n_samples=1000, noise=0.3, shuffle=TRUE, random_state=123, plot=TRUE)
make_spiral <- function(n_samples = 100, shuffle = TRUE, noise = NULL, random_state = NULL, plot = FALSE) {
if (!is.null(random_state)) {
set.seed(random_state)
}
n_samples_per_spiral <- n_samples / 2
max_rotation <- 4 * pi  # how many times we rotate
#  spirals (sing math from make_moons and make_circles)
t1 <- seq(0, max_rotation, length.out = n_samples_per_spiral)
spiral1_x <- t1 * cos(t1)
spiral1_y <- t1 * sin(t1)
t2 <- seq(0, max_rotation, length.out = n_samples_per_spiral)
spiral2_x <- t2 * cos(t2 + pi)
spiral2_y <- t2 * sin(t2 + pi)
# combine the spirals
data <- rbind(cbind(spiral1_x, spiral1_y), cbind(spiral2_x, spiral2_y))
# adding noise
if (!is.null(noise)) {
data <- data + matrix(rnorm(n_samples * 2, sd = noise), ncol = 2)
}
# shuffle data if specified
if (shuffle) {
shuffle_index <- sample(n_samples)
data <- data[shuffle_index, ]
}
# labels for the spirals
labels <- c(rep(1, n_samples_per_spiral), rep(2, n_samples_per_spiral))
if (shuffle) {
labels <- labels[shuffle_index]
}
# plot if specified
if (plot) {
colors <- c("red", "blue")
plot(data, col=colors[labels], xlab = "Feature 1", ylab = "Feature 2", pch = 19)
legend("topright", legend = c("Spiral 1", "Spiral 2"), fill=colors)
}
return(list("X" = data, "y" = labels))
}
make_spiral(n_samples = 500, plot = TRUE, shuffle = FALSE)
make_spiral(n_samples = 200, noise = 0.1, plot = TRUE, shuffle = FALSE)
make_spiral(n_samples = 200, noise = 0.5, random_state = 123, plot = TRUE)
usethis::use_vignette("synthetic_datasets_guide")
